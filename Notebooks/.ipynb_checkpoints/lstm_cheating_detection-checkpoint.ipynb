{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a0658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pydotplus\n",
    "import graphviz\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34a38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    \"\"\"\n",
    "    Handles data setup, directory creation, saving/loading keypoints\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions, data_path, num_sequences=30, sequence_len=30):\n",
    "        self.actions = actions\n",
    "        self.data_path = Path(data_path)\n",
    "        self.num_sequences = num_sequences\n",
    "        self.sequence_len = sequence_len\n",
    "\n",
    "    def _get_next_data_batch_path(self):\n",
    "        \"\"\"Scans base folder for numeric subfolders and returns the next available path\"\"\"\n",
    "        existing = [int(p.name) for p in self.data_path.iterdir() if p.is_dir() and p.name.isdigit()]\n",
    "        next_id = max(existing) + 1 if existing else 1\n",
    "        return self.data_path / str(next_id)\n",
    "\n",
    "    def _create_configs(self):\n",
    "        \"\"\"Returns list of configs for each numeric batch directory\"\"\"\n",
    "        configs = []\n",
    "        for folder in sorted(self.data_path.iterdir()):\n",
    "            if folder.is_dir() and folder.name.isdigit():\n",
    "                first_action_dir = folder / self.actions[0]\n",
    "                if first_action_dir.exists():\n",
    "                    num_seq = len([d for d in first_action_dir.iterdir() if d.is_dir()])\n",
    "                    configs.append({\"path\": folder, \"num_seq\": num_seq})\n",
    "        return configs\n",
    "    \n",
    "    def setup_new_batch(self):\n",
    "        \"\"\"Creates a new batch folder with all subfolders\"\"\"\n",
    "        new_batch_path = self._get_next_data_batch_path()\n",
    "        for action in self.actions:\n",
    "            for seq in range(self.num_sequences):\n",
    "                (new_batch_path / action / str(seq)).mkdir(parents=True, exist_ok=True)\n",
    "        return new_batch_path\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"Ensures all subfolders exist for all configs\"\"\"\n",
    "        for config in self._create_configs():\n",
    "            path = config[\"path\"]\n",
    "            num_seq = config[\"num_seq\"]\n",
    "            for action in self.actions:\n",
    "                for seq in range(num_seq):\n",
    "                    (path / action / str(seq)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save_np_array(self, keypoints, save_path, action, sequence, frame_num):\n",
    "        \"\"\"Saves a single .npy file\"\"\"\n",
    "        npy_path = Path(save_path) / action / str(sequence) / f\"{frame_num}.npy\"\n",
    "        np.save(npy_path, keypoints)\n",
    "\n",
    "    def _load_sequence_data(self, configs):\n",
    "        \"\"\"Loads all sequences and returns X and y\"\"\"\n",
    "        label_map = {label: idx for idx, label in enumerate(self.actions)}\n",
    "        all_sequences, all_labels = [], []\n",
    "\n",
    "        for config in configs:\n",
    "            path = config[\"path\"]\n",
    "            num_sequences = config[\"num_seq\"]\n",
    "\n",
    "            for action in self.actions:\n",
    "                for seq_idx in range(num_sequences):\n",
    "                    window = []\n",
    "                    for frame_idx in range(self.sequence_len):\n",
    "                        file_path = path / action / str(seq_idx) / f\"{frame_idx}.npy\"\n",
    "                        res = np.load(file_path)\n",
    "                        window.append(res)\n",
    "                    all_sequences.append(window)\n",
    "                    all_labels.append(label_map[action])\n",
    "\n",
    "        return np.array(all_sequences), np.array(all_labels)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Returns X_train, y_train, X_val, y_val\"\"\"\n",
    "        X, y = self._load_sequence_data(self._create_configs())\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n",
    "        y_train = to_categorical(y_train)\n",
    "        y_val = to_categorical(y_val)\n",
    "\n",
    "        return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b459a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionRecognizer:\n",
    "    \"\"\"\n",
    "    Detects and classifies actions like 'Paper Swap' using Mediapipe Holistic\n",
    "    It can detect, collect training data and do almost all stuff related to detection!\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes\n",
    "    mp_h = mp.solutions.holistic\n",
    "    mp_d = mp.solutions.drawing_utils\n",
    "    \n",
    "    def __init__(self, data_handler):\n",
    "        self.data_handler = data_handler\n",
    "    \n",
    "    # Mediapipe setup\n",
    "    def mediapipe_detection(self, image, model):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = model.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        return image, results\n",
    "    \n",
    "    # Draw landmarks\n",
    "    def draw_landmarks(self, image, results):\n",
    "        # Face connections\n",
    "        self.mp_d.draw_landmarks(image, results.face_landmarks, self.mp_h.FACEMESH_CONTOURS, \n",
    "                                 self.mp_d.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                                 self.mp_d.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 ) \n",
    "        # Pose connections\n",
    "        self.mp_d.draw_landmarks(image, results.pose_landmarks, self.mp_h.POSE_CONNECTIONS,\n",
    "                                 self.mp_d.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                                 self.mp_d.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 ) \n",
    "        # Left hand connections\n",
    "        self.mp_d.draw_landmarks(image, results.left_hand_landmarks, self.mp_h.HAND_CONNECTIONS, \n",
    "                                 self.mp_d.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                                 self.mp_d.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 ) \n",
    "        # Right hand connections  \n",
    "        self.mp_d.draw_landmarks(image, results.right_hand_landmarks, self.mp_h.HAND_CONNECTIONS, \n",
    "                                 self.mp_d.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                                 self.mp_d.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "    # Extract keypoints\n",
    "    def extract_keypoints(self, results):\n",
    "        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "        face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "        return np.concatenate([pose, face, lh, rh])\n",
    "    \n",
    "    # Video capture\n",
    "    def video_capture(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        return cap\n",
    "    \n",
    "    # Test landmarks\n",
    "    def test_landmarks(self):\n",
    "        cap = self.video_capture()\n",
    "        \n",
    "        with self.mp_h.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "            while cap.isOpened():\n",
    "                \n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "                \n",
    "                # Make detection\n",
    "                image, results = self.mediapipe_detection(frame, holistic)\n",
    "                \n",
    "                # Draw landmarks\n",
    "                self.draw_landmarks(image, results)\n",
    "                \n",
    "                # Show to the screen\n",
    "                cv2.imshow(\"OpenCV Feed\", image)\n",
    "                \n",
    "                # Break (press q to quit)\n",
    "                if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # Collect keypoints from webcam and save .npy files\n",
    "    def collect_keypoints_to_path(self):\n",
    "        \n",
    "        # Set up directories\n",
    "        save_path = self.data_handler.setup_new_batch()\n",
    "        self.data_handler.setup_directories()\n",
    "        \n",
    "        cap = self.video_capture()\n",
    "\n",
    "        with self.mp_h.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "            for action in self.data_handler.actions:\n",
    "                for sequence in range(self.data_handler.num_sequences):\n",
    "                    for frame_num in range(self.data_handler.sequence_len):\n",
    "                        \n",
    "                        # Read feed\n",
    "                        ret, frame = cap.read()\n",
    "\n",
    "                        image, results = self.mediapipe_detection(frame, holistic)\n",
    "                        self.draw_landmarks(image, results)\n",
    "\n",
    "                        # Display info\n",
    "                        if frame_num == 0:\n",
    "                            cv2.putText(image, f\"Starting {action} #{sequence}\", (65,200),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 3, (0,255,0), 8, cv2.LINE_AA)\n",
    "                            cv2.imshow(\"OpenCV Feed\", image)\n",
    "                            cv2.waitKey(2000)\n",
    "                        \n",
    "                        else:\n",
    "                            cv2.putText(image, f\"{action} #{sequence}\", (30,40),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2, cv2.LINE_AA)\n",
    "                            cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "                        # Save the keypoints\n",
    "                        keypoints = self.extract_keypoints(results)\n",
    "                        self.data_handler.save_np_array(keypoints, save_path, action, sequence, frame_num)\n",
    "\n",
    "                        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                            cap.release()\n",
    "                            cv2.destroyAllWindows()\n",
    "                            return\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4383b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDetectionModel():\n",
    "    \"\"\"\n",
    "    Model for detecting human actions from pose data sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, actions, input_shape=(30, 1662), learning_rate=0.01):\n",
    "        self.actions = actions\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=True, activation=\"relu\", input_shape=self.input_shape),\n",
    "            LSTM(32, return_sequences=False),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dense(32, activation=\"relu\"),\n",
    "            Dense(len(self.actions), activation=\"softmax\")\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=[\"categorical_accuracy\"]\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val, y_val, epochs=100):\n",
    "        self.model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs)\n",
    "\n",
    "    def predict_proba(self, sequence):\n",
    "        \"\"\"Predict class probabilities for a single sequence (shape: 30Ã—1662).\"\"\"\n",
    "        sequence = np.expand_dims(sequence, axis=0)\n",
    "        \n",
    "        if sequence.shape != self.input_shape:\n",
    "            raise ValueError(f\"Expected input shape {self.input_shape}, got {sequence.shape}\")\n",
    "        \n",
    "        return self.model.predict(sequence, verbose=0)[0]\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        \"\"\"Return the most likely action class index.\"\"\"\n",
    "        return int(np.argmax(self.predict_proba(sequence)))\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate the model and print accuracy.\"\"\"\n",
    "        y_pred = np.argmax(self.model.predict(X_test, verbose=0), axis=1)\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Test Accuracy: {acc:.4f}\")\n",
    "        return acc\n",
    "\n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    def save(self, file_name):\n",
    "        \"\"\"Save model to 'Models/' directory (relative to project root).\"\"\"\n",
    "        save_path = Path.cwd().parent / \"Models\" / file_name\n",
    "        self.model.save(save_path)\n",
    "        \n",
    "    def load(self, file_name):\n",
    "        \"\"\"Load a saved model.\"\"\"\n",
    "        load_path = Path.cwd().parent / \"Models\" / file_name\n",
    "        self.model = tf.keras.models.load_model(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03a99aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1491693281.py, line 99)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 99\u001b[1;36m\u001b[0m\n\u001b[1;33m    collect_user_info():\u001b[0m\n\u001b[1;37m                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ExamRecord:\n",
    "    \"\"\"\n",
    "    This class starts the exam!\n",
    "    \"\"\"\n",
    "    \n",
    "    name: str\n",
    "    avg_prev_score: float\n",
    "    paper_swap: int\n",
    "    side_glance: int\n",
    "    score: float\n",
    "    ans_time: float\n",
    "        \n",
    "    def __init__(self, recognizer, action_detection_model, d_tree):\n",
    "        self.recognizer = recognizer\n",
    "        self.action_detection_model = action_detection_model\n",
    "        self.d_tree = d_tree\n",
    "\n",
    "    # Visualization function\n",
    "    def prob_viz(self, res, input_frame):\n",
    "        colors = [(125, 142, 91), (85, 43, 90), (89, 162, 244), (81, 75, 188)]\n",
    "        output_frame = input_frame.copy()\n",
    "        for num, prob in enumerate(res):\n",
    "            cv2.rectangle(output_frame, (0,60 + num * 38), (int(prob * 220), 95 + num * 40), colors[num], -1)\n",
    "            cv2.putText(output_frame, self.recognizer.data_handler.actions[num], (0, 85 + num * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255 ,255 ,255), 2, cv2.LINE_AA)    \n",
    "        \n",
    "        return output_frame \n",
    "\n",
    "    # Extract info\n",
    "    def extract_info(self, acts):\n",
    "        self.paper_swap = np.sum(acts == \"Paper Swap\")\n",
    "        self.side_glance = np.sum(acts == \"Side Glance\")\n",
    "    \n",
    "    # Sentence prediction\n",
    "    def detection_sentence(self, record):\n",
    "        if self.d_tree.predict(record) == 1:\n",
    "            return \"unfortunately, is suspicious of cheating!\"\n",
    "        else:\n",
    "            return \"is not suspicious of cheating.\"\n",
    "    \n",
    "    # Detection method\n",
    "    def detect(self):\n",
    "            # Define variables\n",
    "            sequence, sentence, acts, predictions, threshold = [], [], [], [], 0.5\n",
    "                \n",
    "            # Video capture\n",
    "            cap = self.recognizer.video_capture()\n",
    "\n",
    "            # Set mediapipe model \n",
    "            with self.recognizer.mp_h.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "                while \"End\" not in acts:\n",
    "                    \n",
    "                    # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "                    \n",
    "                    # Make detections\n",
    "                    image, results = self.recognizer.mediapipe_detection(frame, holistic)\n",
    "                    \n",
    "                    # Draw landmarks\n",
    "                    self.recognizer.draw_landmarks(image, results)\n",
    "                    \n",
    "                    # Prediction\n",
    "                    keypoints = self.recognizer.extract_keypoints(results)\n",
    "                    sequence.append(keypoints)\n",
    "                    sequence = sequence[-30:]\n",
    "                    if len(sequence) == 30:\n",
    "                        logits = self.action_detection_model.model(np.expand_dims(sequence, axis = 0))[0]\n",
    "                        res = tf.nn.softmax(logits)\n",
    "                        predictions.append(np.argmax(res))\n",
    "                    \n",
    "                    # Visualizations\n",
    "                        if np.unique(predictions[-10:])[0] == np.argmax(res): \n",
    "                            if res[np.argmax(res)] > threshold:    \n",
    "                                if len(sentence) > 0: \n",
    "                                    if self.recognizer.data_handler.actions[np.argmax(res)] != sentence[-1]:\n",
    "                                        acts.append(sentence[-1])\n",
    "                                        sentence.append(self.recognizer.data_handler.actions[np.argmax(res)])\n",
    "                                else:\n",
    "                                    sentence.append(self.recognizer.data_handler.actions[np.argmax(res)])\n",
    "                                    acts.append(sentence[-1])\n",
    "                        if len(sentence) > 5: \n",
    "                            sentence = sentence[-5:]\n",
    "                        image = self.prob_viz(res, image)\n",
    "                    cv2.rectangle(image, (0,0), (2496, 40), (90, 77, 43), -1)\n",
    "                    cv2.putText(image, \"  \".join(sentence), (3,30), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Show to screen\n",
    "                    cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "                    # Break with typing \"q\"\n",
    "                    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "                        break\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "            \n",
    "            return acts\n",
    "        \n",
    "    # Collect information from the user\n",
    "    def collect_user_info(self):\n",
    "        self.name = input(\"Please enter your name: \")\n",
    "        self.av_prev_score = float(input(\"Please enter average of your previous scores: \"))\n",
    "    \n",
    "    # Countdown\n",
    "    def countdown(self, t=5):\n",
    "        print(\"\\nThe exam will be started in:\")\n",
    "        while t >= 0:\n",
    "            timer = \"{:02d} Seconds\".format(t)\n",
    "            print(timer)\n",
    "            time.sleep(1)\n",
    "            t -= 1\n",
    "        print(\"\\nStart!\")\n",
    "        \n",
    "    def create_df(self):\n",
    "        return pd.DataFrame({\n",
    "            \"Name\" : self.name,\n",
    "            \"Av Prev Scores\" : self.av_prev_score,\n",
    "            \"Paper Swap\" : self.paper_swap,\n",
    "            \"Side Glance\" : self.side_glance,\n",
    "            \"Score\" : self.score,\n",
    "            \"Ans Time\" : self.ans_time,\n",
    "            \"Score / Ans Time\" : self.score / self.ans_time\n",
    "            }, index = [0])\n",
    "\n",
    "    # 10 Steps to start an exam!\n",
    "    def start_exam(self):\n",
    "        # 1. Staring\n",
    "        self.collect_user_info()\n",
    "\n",
    "        # 3. Countdown and start time measuement\n",
    "        self.countdown()\n",
    "        start = time.time() \n",
    "\n",
    "        # 4. Time to detect!\n",
    "        acts = self.detect()\n",
    "            \n",
    "        # 5. Calculate total time\n",
    "        end = time.time()\n",
    "        self.ans_time = round((end - start) / 60, 2)\n",
    "\n",
    "        self.score = input(\"Enter the score: \")\n",
    "        \n",
    "        record = self.create_df()\n",
    "        \n",
    "        # 9. Make prediction using decision tree\n",
    "            # Final Prediction\n",
    "        final_pred = self.detection_sentence(record)\n",
    "        \n",
    "        # 10. Return all results\n",
    "        # Final status\n",
    "        status = f\"\\n\\n\\n{self.name}'s test status: \\nScore: {self.score}\\n{self.name} has changed paper for --{self.paper_swap}-- time\\s\\nand looked aside for --{self.side_glance}-- time\\s\\n{self.name} finished the test in {self.ans_time} minutes\\nThe score of this test is {self.score - self.av_prev_score} points different from {self.name}'s previous average scores.\\n\\nFinally, by detection of AI,\\n{self.name} {final_pred}\"\n",
    "        \n",
    "        # Prediction path plot\n",
    "        columns = [\"Av Prev Scores\", \"Paper Swap\", \"Side Glance\", \"Score\", \"Ans Time\", \"Score / Ans Time\", \"Cheat\"]\n",
    "        dot_data = tree.export_graphviz(self.d_tree, out_file = None,\n",
    "                                    feature_names = columns[0 : 6],\n",
    "                                    class_names = [\"Not Cheat\", \"Cheat\"],\n",
    "                                    filled = True, rounded = True,\n",
    "                                    impurity = False,\n",
    "                                    special_characters = True)\n",
    "        graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "        for node in graph.get_node_list():\n",
    "            if node.get_attributes().get(\"label\") is None:\n",
    "                continue\n",
    "            if \"samples = \" in node.get_attributes()[\"label\"]:\n",
    "                labels = node.get_attributes()[\"label\"].split(\"<br/>\")\n",
    "                for i, label in enumerate(labels):\n",
    "                    if label.startswith(\"samples = \"):\n",
    "                        labels[i] = \"samples = 0\"\n",
    "                node.set(\"label\", \"<br/>\".join(labels))\n",
    "                node.set_fillcolor(\"white\")\n",
    "        samples = record\n",
    "        decision_paths = self.d_tree.decision_path(samples)\n",
    "        for decision_path in decision_paths:\n",
    "            for n, node_value in enumerate(decision_path.toarray()[0]):\n",
    "                if node_value == 0:\n",
    "                    continue\n",
    "                node = graph.get_node(str(n))[0]            \n",
    "                node.set_fillcolor(\"lightgreen\")\n",
    "                labels = node.get_attributes()[\"label\"].split(\"<br/>\")\n",
    "                for i, label in enumerate(labels):\n",
    "                    if label.startswith(\"samples = \"):\n",
    "                        labels[i] = \"samples = {}\".format(int(label.split(\"=\")[1]) + 1)\n",
    "                node.set(\"label\", \"<br/>\".join(labels))\n",
    "                # Final returns\n",
    "        display(graphviz.Source(graph.to_string()))\n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c07c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"\n",
    "    Loads the models\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    def load_d_tree(file_name):\n",
    "        project_root = Path.cwd().parent\n",
    "        base_path = project_root / \"Models\"\n",
    "        d_tree = pickle.load(open(base_path / file_name, \"rb\"))\n",
    "        \n",
    "        return d_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af5ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define actions and path\n",
    "actions = [\"End\", \"Normal\", \"Paper Swap\", \"Side Glance\"]\n",
    "data_path = Path.cwd().parent / \"Actions Dataset\"\n",
    "\n",
    "# Instantiate handler\n",
    "data_handler = DataHandler(\n",
    "    actions=actions,\n",
    "    data_path=data_path,\n",
    ")\n",
    "\n",
    "# Instantiate recognizer\n",
    "recognizer = ActionRecognizer(data_handler=data_handler)\n",
    "\n",
    "# Start data collection (will save keypoints to .npy)\n",
    "recognizer.collect_keypoints_to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b69754e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "actions = [\"End\", \"Normal\", \"Paper Swap\", \"Side Glance\"]\n",
    "\n",
    "# Load models\n",
    "model_loader = ModelLoader\n",
    "d_tree = model_loader.load_d_tree(\"decision_tree.sav\")\n",
    "action_detection_model = model_loader.load_action_detection_model(\"lstm.h5\")\n",
    "\n",
    "# Initialize other classes\n",
    "action_gesture_recognizer = ActionGestureRecognizer(actions)\n",
    "action_detection_model = ActionDetectionModel(actions)\n",
    "exam_system = ExamSystem(action_gesture_recognizer, action_detection_model, d_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5379c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test landmarks\n",
    "action_gesture_recognizer.test_landmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6db4fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data\n",
    "project_root = Path.cwd().parent\n",
    "base_path = project_root / \"Actions Dataset\"\n",
    "new_batch_path = action_gesture_recognizer._setup_collection_dirs(base_path)\n",
    "action_gesture_recognizer.collect_keypoints_to_path(new_batch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d62df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 5s 5s/step - loss: 1.4011 - categorical_accuracy: 0.0667 - val_loss: 1.4186 - val_categorical_accuracy: 0.2000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.3826 - categorical_accuracy: 0.2667 - val_loss: 1.4506 - val_categorical_accuracy: 0.2000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.3838 - categorical_accuracy: 0.3333 - val_loss: 1.3993 - val_categorical_accuracy: 0.4000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.3917 - categorical_accuracy: 0.3333 - val_loss: 1.3276 - val_categorical_accuracy: 0.4000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.4205 - categorical_accuracy: 0.2000 - val_loss: 1.4224 - val_categorical_accuracy: 0.2000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.3649 - categorical_accuracy: 0.2667 - val_loss: 1.4750 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.3992 - categorical_accuracy: 0.2000 - val_loss: 1.4083 - val_categorical_accuracy: 0.2000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3589 - categorical_accuracy: 0.4000 - val_loss: 1.3777 - val_categorical_accuracy: 0.4000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.3623 - categorical_accuracy: 0.3333 - val_loss: 1.4328 - val_categorical_accuracy: 0.2000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.4152 - categorical_accuracy: 0.2000 - val_loss: 1.4013 - val_categorical_accuracy: 0.2000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3800 - categorical_accuracy: 0.2667 - val_loss: 1.4240 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.3862 - categorical_accuracy: 0.1333 - val_loss: 1.4988 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.4226 - categorical_accuracy: 0.1333 - val_loss: 1.4195 - val_categorical_accuracy: 0.2000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.3929 - categorical_accuracy: 0.2667 - val_loss: 1.4180 - val_categorical_accuracy: 0.2000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.3809 - categorical_accuracy: 0.2667 - val_loss: 1.4066 - val_categorical_accuracy: 0.2000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3838 - categorical_accuracy: 0.2667 - val_loss: 1.4106 - val_categorical_accuracy: 0.2000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.3834 - categorical_accuracy: 0.2667 - val_loss: 1.4176 - val_categorical_accuracy: 0.2000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.3802 - categorical_accuracy: 0.2667 - val_loss: 1.4231 - val_categorical_accuracy: 0.2000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.3796 - categorical_accuracy: 0.2667 - val_loss: 1.4257 - val_categorical_accuracy: 0.2000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3785 - categorical_accuracy: 0.2667 - val_loss: 1.4321 - val_categorical_accuracy: 0.2000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.3803 - categorical_accuracy: 0.2667 - val_loss: 1.4337 - val_categorical_accuracy: 0.2000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.3766 - categorical_accuracy: 0.2667 - val_loss: 1.4314 - val_categorical_accuracy: 0.2000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3741 - categorical_accuracy: 0.2667 - val_loss: 1.4313 - val_categorical_accuracy: 0.2000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.3732 - categorical_accuracy: 0.3333 - val_loss: 1.4331 - val_categorical_accuracy: 0.2000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.3715 - categorical_accuracy: 0.4000 - val_loss: 1.4420 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.3782 - categorical_accuracy: 0.2000 - val_loss: 1.4359 - val_categorical_accuracy: 0.2000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3700 - categorical_accuracy: 0.4000 - val_loss: 1.4367 - val_categorical_accuracy: 0.2000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.3722 - categorical_accuracy: 0.3333 - val_loss: 1.4379 - val_categorical_accuracy: 0.2000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.3710 - categorical_accuracy: 0.3333 - val_loss: 1.4476 - val_categorical_accuracy: 0.2000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.3652 - categorical_accuracy: 0.4667 - val_loss: 1.4570 - val_categorical_accuracy: 0.2000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.3783 - categorical_accuracy: 0.2667 - val_loss: 1.4405 - val_categorical_accuracy: 0.2000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3702 - categorical_accuracy: 0.2667 - val_loss: 1.4394 - val_categorical_accuracy: 0.2000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.3727 - categorical_accuracy: 0.2667 - val_loss: 1.4390 - val_categorical_accuracy: 0.2000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.3727 - categorical_accuracy: 0.2667 - val_loss: 1.4391 - val_categorical_accuracy: 0.2000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.3725 - categorical_accuracy: 0.2667 - val_loss: 1.4392 - val_categorical_accuracy: 0.2000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.3726 - categorical_accuracy: 0.2667 - val_loss: 1.4393 - val_categorical_accuracy: 0.2000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.3718 - categorical_accuracy: 0.3333 - val_loss: 1.4394 - val_categorical_accuracy: 0.2000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.3710 - categorical_accuracy: 0.3333 - val_loss: 1.4395 - val_categorical_accuracy: 0.2000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.3703 - categorical_accuracy: 0.3333 - val_loss: 1.4397 - val_categorical_accuracy: 0.2000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.3697 - categorical_accuracy: 0.3333 - val_loss: 1.4398 - val_categorical_accuracy: 0.2000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.3690 - categorical_accuracy: 0.3333 - val_loss: 1.4399 - val_categorical_accuracy: 0.2000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.3683 - categorical_accuracy: 0.3333 - val_loss: 1.4400 - val_categorical_accuracy: 0.2000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 1.3677 - categorical_accuracy: 0.3333 - val_loss: 1.4399 - val_categorical_accuracy: 0.2000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.3671 - categorical_accuracy: 0.3333 - val_loss: 1.4397 - val_categorical_accuracy: 0.2000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.3665 - categorical_accuracy: 0.3333 - val_loss: 1.4396 - val_categorical_accuracy: 0.2000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.3658 - categorical_accuracy: 0.3333 - val_loss: 1.4396 - val_categorical_accuracy: 0.2000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 1.3652 - categorical_accuracy: 0.3333 - val_loss: 1.4385 - val_categorical_accuracy: 0.2000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.3645 - categorical_accuracy: 0.3333 - val_loss: 1.4377 - val_categorical_accuracy: 0.2000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1.3639 - categorical_accuracy: 0.3333 - val_loss: 1.4370 - val_categorical_accuracy: 0.2000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.3633 - categorical_accuracy: 0.3333 - val_loss: 1.4364 - val_categorical_accuracy: 0.2000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.3626 - categorical_accuracy: 0.3333 - val_loss: 1.4360 - val_categorical_accuracy: 0.2000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.3618 - categorical_accuracy: 0.3333 - val_loss: 1.4353 - val_categorical_accuracy: 0.2000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.3612 - categorical_accuracy: 0.4000 - val_loss: 1.4347 - val_categorical_accuracy: 0.4000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 1.3606 - categorical_accuracy: 0.3333 - val_loss: 1.4333 - val_categorical_accuracy: 0.2000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 1.3599 - categorical_accuracy: 0.2667 - val_loss: 1.4316 - val_categorical_accuracy: 0.2000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 118ms/step - loss: 1.3591 - categorical_accuracy: 0.2667 - val_loss: 1.4295 - val_categorical_accuracy: 0.2000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 1.3580 - categorical_accuracy: 0.2667 - val_loss: 1.4270 - val_categorical_accuracy: 0.2000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.3567 - categorical_accuracy: 0.3333 - val_loss: 1.4244 - val_categorical_accuracy: 0.2000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 1.3551 - categorical_accuracy: 0.4000 - val_loss: 1.4217 - val_categorical_accuracy: 0.4000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.3532 - categorical_accuracy: 0.3333 - val_loss: 1.4179 - val_categorical_accuracy: 0.4000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.3511 - categorical_accuracy: 0.3333 - val_loss: 1.4128 - val_categorical_accuracy: 0.4000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.3487 - categorical_accuracy: 0.3333 - val_loss: 1.4076 - val_categorical_accuracy: 0.4000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.3461 - categorical_accuracy: 0.4000 - val_loss: 1.4024 - val_categorical_accuracy: 0.4000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.3433 - categorical_accuracy: 0.4667 - val_loss: 1.3968 - val_categorical_accuracy: 0.4000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.3404 - categorical_accuracy: 0.2667 - val_loss: 1.3911 - val_categorical_accuracy: 0.4000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.3377 - categorical_accuracy: 0.2667 - val_loss: 1.3857 - val_categorical_accuracy: 0.4000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 1.3350 - categorical_accuracy: 0.2667 - val_loss: 1.3825 - val_categorical_accuracy: 0.4000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.3324 - categorical_accuracy: 0.4000 - val_loss: 1.3800 - val_categorical_accuracy: 0.4000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.3295 - categorical_accuracy: 0.6000 - val_loss: 1.3750 - val_categorical_accuracy: 0.4000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.3262 - categorical_accuracy: 0.4000 - val_loss: 1.3742 - val_categorical_accuracy: 0.6000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.3223 - categorical_accuracy: 0.5333 - val_loss: 1.3730 - val_categorical_accuracy: 0.6000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.3181 - categorical_accuracy: 0.6000 - val_loss: 1.3680 - val_categorical_accuracy: 0.4000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.3132 - categorical_accuracy: 0.5333 - val_loss: 1.3677 - val_categorical_accuracy: 0.6000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.3079 - categorical_accuracy: 0.4667 - val_loss: 1.3612 - val_categorical_accuracy: 0.4000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.3029 - categorical_accuracy: 0.6000 - val_loss: 1.3703 - val_categorical_accuracy: 0.4000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.3014 - categorical_accuracy: 0.4000 - val_loss: 1.3748 - val_categorical_accuracy: 0.4000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.3084 - categorical_accuracy: 0.4000 - val_loss: 1.3524 - val_categorical_accuracy: 0.4000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.2854 - categorical_accuracy: 0.4667 - val_loss: 1.3563 - val_categorical_accuracy: 0.4000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.2847 - categorical_accuracy: 0.4667 - val_loss: 1.3717 - val_categorical_accuracy: 0.4000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 1.3080 - categorical_accuracy: 0.4667 - val_loss: 1.3617 - val_categorical_accuracy: 0.4000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1.2835 - categorical_accuracy: 0.4667 - val_loss: 1.3528 - val_categorical_accuracy: 0.4000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.2739 - categorical_accuracy: 0.4667 - val_loss: 1.3616 - val_categorical_accuracy: 0.4000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.2782 - categorical_accuracy: 0.5333 - val_loss: 1.3436 - val_categorical_accuracy: 0.4000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.2627 - categorical_accuracy: 0.5333 - val_loss: 1.3493 - val_categorical_accuracy: 0.4000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1.2663 - categorical_accuracy: 0.4667 - val_loss: 1.3345 - val_categorical_accuracy: 0.4000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.2567 - categorical_accuracy: 0.5333 - val_loss: 1.3822 - val_categorical_accuracy: 0.2000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.3049 - categorical_accuracy: 0.4000 - val_loss: 1.3131 - val_categorical_accuracy: 0.4000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1.2385 - categorical_accuracy: 0.5333 - val_loss: 1.3582 - val_categorical_accuracy: 0.4000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.2679 - categorical_accuracy: 0.4667 - val_loss: 1.3160 - val_categorical_accuracy: 0.4000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.2589 - categorical_accuracy: 0.4000 - val_loss: 1.2895 - val_categorical_accuracy: 0.4000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.2152 - categorical_accuracy: 0.5333 - val_loss: 1.3049 - val_categorical_accuracy: 0.4000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.2557 - categorical_accuracy: 0.4667 - val_loss: 1.4233 - val_categorical_accuracy: 0.2000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.3144 - categorical_accuracy: 0.4000 - val_loss: 1.4890 - val_categorical_accuracy: 0.2000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.3723 - categorical_accuracy: 0.2667 - val_loss: 1.4785 - val_categorical_accuracy: 0.2000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.3689 - categorical_accuracy: 0.2667 - val_loss: 1.4375 - val_categorical_accuracy: 0.2000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.3382 - categorical_accuracy: 0.2667 - val_loss: 1.3802 - val_categorical_accuracy: 0.4000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1.2901 - categorical_accuracy: 0.4000 - val_loss: 1.3594 - val_categorical_accuracy: 0.4000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1.2759 - categorical_accuracy: 0.4667 - val_loss: 1.3888 - val_categorical_accuracy: 0.2000\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step - loss: 1.3061 - categorical_accuracy: 0.3333 - val_loss: 1.4048 - val_categorical_accuracy: 0.2000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.3217 - categorical_accuracy: 0.4000 - val_loss: 1.3984 - val_categorical_accuracy: 0.2000\n"
     ]
    }
   ],
   "source": [
    "# Load the data and train the model\n",
    "# This doesn't do anything because here we're using a very small dataset\n",
    "X_train, y_train, X_test, y_test = action_gesture_recognizer.load_data()\n",
    "action_detection_model.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "159c8ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 458,852\n",
      "Trainable params: 458,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of the model\n",
    "action_detection_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17b105f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### Exam ######################\n",
      "\n",
      "Please enter your name: jlk\n",
      "Please enter average of your previous scores: 9\n",
      "\n",
      "The exam will be started in:\n",
      "05 Seconds\n",
      "04 Seconds\n",
      "03 Seconds\n",
      "02 Seconds\n",
      "01 Seconds\n",
      "00 Seconds\n",
      "\n",
      "Start!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ExamSystem' object has no attribute 'actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TIME TO TAKE AN EXAM!\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mexam_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_exam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[9], line 124\u001b[0m, in \u001b[0;36mExamSystem.start_exam\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m ans_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m((end \u001b[38;5;241m-\u001b[39m start) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# 6. Extract informations from the video\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m paper_swap, side_glance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# 7. Input score\u001b[39;00m\n\u001b[0;32m    127\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enter the score: \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m, in \u001b[0;36mExamSystem.extract_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_info\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(actions \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPaper Swap\u001b[39m\u001b[38;5;124m\"\u001b[39m), np\u001b[38;5;241m.\u001b[39msum(actions \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSide Glance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ExamSystem' object has no attribute 'actions'"
     ]
    }
   ],
   "source": [
    "# TIME TO TAKE AN EXAM!\n",
    "print(exam_system.start_exam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3969c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
